{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBXR6oa6hE883kSkNS6Y0a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/canerskrc/LLM-GPT/blob/main/Scikit_LLM_E_Mail_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6vh_o2n93tt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn_llm import LLMClassifier\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import openai\n",
        "from googleapiclient.discovery import build\n",
        "from google_auth_oauthlib.flow import InstalledAppFlow\n",
        "from google.auth.transport.requests import Request\n",
        "import pickle\n",
        "import os.path\n",
        "\n",
        "openai.api_key = 'YOUR_OPENAI_API_KEY'\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def authenticate_google():\n",
        "    SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
        "    creds = None\n",
        "    if os.path.exists('token.pickle'):\n",
        "        with open('token.pickle', 'rb') as token:\n",
        "            creds = pickle.load(token)\n",
        "    if not creds or not creds.valid:\n",
        "        if creds and creds.expired and creds.refresh_token:\n",
        "            creds.refresh(Request())\n",
        "        else:\n",
        "            flow = InstalledAppFlow.from_client_secrets_file(\n",
        "                'credentials.json', SCOPES)\n",
        "            creds = flow.run_local_server(port=0)\n",
        "        with open('token.pickle', 'wb') as token:\n",
        "            pickle.dump(creds, token)\n",
        "    service = build('gmail', 'v1', credentials=creds)\n",
        "    return service\n",
        "\n",
        "#Gmail API\n",
        "def fetch_emails(service, user_id='me', label_ids=None):\n",
        "    emails = []\n",
        "    results = service.users().messages().list(userId=user_id, labelIds=label_ids).execute()\n",
        "    messages = results.get('messages', [])\n",
        "    for message in messages:\n",
        "        msg = service.users().messages().get(userId=user_id, id=message['id']).execute()\n",
        "        payload = msg['payload']\n",
        "        headers = payload['headers']\n",
        "        subject = [i['value'] for i in headers if i[\"name\"] == \"Subject\"]\n",
        "        body = ''\n",
        "        if 'parts' in payload:\n",
        "            for part in payload['parts']:\n",
        "                if part['mimeType'] == 'text/plain':\n",
        "                    body = part['body']['data']\n",
        "                    break\n",
        "        emails.append({'subject': subject[0] if subject else '', 'body': body})\n",
        "    return emails\n",
        "\n",
        "#Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "#Main Function\n",
        "def main():\n",
        "    # Google API\n",
        "    service = authenticate_google()\n",
        "\n",
        "    emails = fetch_emails(service)\n",
        "    df = pd.DataFrame(emails)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df['body'], df['subject'], test_size=0.2, random_state=42)\n",
        "\n",
        "    X_train = X_train.apply(preprocess_text)\n",
        "    X_test = X_test.apply(preprocess_text)\n",
        "\n",
        "    #Vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "    # LLMClassifier\n",
        "    llm_classifier = LLMClassifier(openai.Completion.create)\n",
        "    llm_classifier.fit(X_train_vec, y_train)\n",
        "    y_pred = llm_classifier.predict(X_test_vec)\n",
        "\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "    #New E-Mail\n",
        "    new_email = \"Müsait olduğun bir saat aralığında kahve içelim mi?\"\n",
        "    new_email_processed = preprocess_text(new_email)\n",
        "    new_email_vec = vectorizer.transform([new_email_processed])\n",
        "    predicted_category = llm_classifier.predict(new_email_vec)\n",
        "\n",
        "    print(f\"Yeni e-posta kategorisi: {predicted_category[0]}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}